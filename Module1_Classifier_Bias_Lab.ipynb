{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifier_Bias_Lab",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdenShaveet/Disclosure-Curriculum/blob/main/Module1_Classifier_Bias_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module Exercise: Train a Model... Perpetuate Bias?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Script Description:** Trains a simple neural network using synthetic patient satisfaction data\n",
        "\n",
        "**Script Attributions:** Script adapted from the bias lab from [Crash Course AI Episode #19](https://www.youtube.com/watch?v=_DZJV9ey1nE&list=PL8dPuuaLjXtO65LeD2p4_Sb5XQ51par_b&index=20), developed by the awesome folks at [Crash Course](https://thecrashcourse.com/topic/ai/). Text and variables have been altered to relfect a different case scenario; however, *all executable values and models remain unchanged.*\n",
        "\n",
        "**Instructions:** Read each text block and run each corresponding code block to generate a simple neural net using synthetic patient satisfaction data. For help with Python and Colab, see the Python Help page at the MDSD4Health curriculum website."
      ],
      "metadata": {
        "id": "AnOcLOS0FLo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our Scenario"
      ],
      "metadata": {
        "id": "JYgL3eqCwEOe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpYhUfZvgdGd"
      },
      "source": [
        "In this exercise, we are going to build a very simple ML classifier to determine which health service programs are eligible to recieve a new funding award that rewards high patient satisfaction in our state based on satisfaction and experience data. This is a simple demonstration exercise, so we will use synthetic \"survey data\" to train a neural network to predict satisfaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rkCNpH_C8wF"
      },
      "source": [
        "# Step 1. Input survey results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfxAD9XY53Re"
      },
      "source": [
        "Instead of importing a dataset for this exercise, let's create our own! We'll be training our model using data from two fictitious care service entities. Because our state is comrpised of both rural and urban areas, we want this to be represented in our dataset:\n",
        "\n",
        "*   **Service Entity 1 (S1):** a practice based in a rural part of the state (sparsely populated areas outside of city and city-like areas)\n",
        "*   **Service Entity 2 (S2):** a practice based in an urban part of the state (densely populated, city and city-like areas)\n",
        "\n",
        "The funders chose *four* care delivery characteristics upon which the model should assess funding eligibility: patient-centered care, culturally-responsive care, timely care, and care offered online to increase access (virtual/telehealth). Our label (target variable) is whether people are satisfied with the care they recieved.\n",
        "\n",
        "For the data collection part of this process, let's pretend we gave a five-item survey of yes/no questions corresponding to these features to 30 patients who use one service entity or the other about their experiences with the entity over the last year:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   **Over the last year, did you find that the care you recieved from this entity was often:**\n",
        "*   *Tailored to your needs as a patient?* (patient-centered, Y/N)\n",
        "*   *Responsive and respectful of your cultures and customs?* (culturally-responsive, Y/N)\n",
        "*   *Delivered in a timely fashion?* (timely, Y/N)\n",
        "*   *Offered to you frequently online via telehealth?* (offered online, Y/N)\n",
        "\n",
        "2. **Overall, were you satisfied with the care you recieved from this entity over the last year?** (satisfaction, Y/N)"
      ],
      "metadata": {
        "id": "y7wVNaoFekVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert the responses into a dataset of features and labels, we compiled everyone’s answers into a list, where every row is one patient's survey response. "
      ],
      "metadata": {
        "id": "YkZBU5g2epbT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SehUtVTQQup_"
      },
      "source": [
        "import numpy as np\n",
        "# Column names:  Offered online, Patient-centered, Culturally responsive, Timely, Satisfaction\n",
        "survey = np.array([\n",
        "  [1, 0, 1, 1, 1],  #     Offered online, Not patient-centered, Culturally responsive, Timely,     Satisfied\n",
        "  [1, 1, 1, 1, 1],  #     Offered online,     Patient-centered, Culturally responsive, Timely,     Satisfied\n",
        "  [1, 0, 1, 0, 1],  #     Offered online, Not patient-centered, Culturally responsive, Not timely, Satisfied\n",
        "  [0, 0, 1, 0, 0],  # Not offered online, Not patient-centered, Culturally responsive, Not timely, Not satisfied\n",
        "  [0, 1, 0, 1, 0],  # ...\n",
        "  [0, 0, 0, 1, 0],\n",
        "  [1, 1, 0, 0, 1],\n",
        "  [0, 1, 0, 0, 0],\n",
        "  [0, 1, 0, 1, 0],\n",
        "  [0, 1, 0, 0, 0],\n",
        "  [1, 0, 1, 1, 1],\n",
        "  [0, 1, 1, 1, 0],\n",
        "  [1, 0, 1, 0, 1],\n",
        "  [0, 0, 1, 0, 0],\n",
        "  [0, 1, 0, 1, 0],\n",
        "  [0, 0, 0, 1, 0],\n",
        "  [1, 1, 0, 0, 1],\n",
        "  [0, 0, 0, 0, 0],\n",
        "  [1, 0, 1, 1, 1],\n",
        "  [1, 1, 1, 1, 0],\n",
        "  [1, 0, 1, 0, 1],\n",
        "  [1, 1, 1, 0, 1],\n",
        "  [0, 0, 0, 0, 1],\n",
        "  [0, 0, 0, 1, 1],\n",
        "  [0, 0, 1, 1, 1],\n",
        "  [0, 1, 1, 1, 1]\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then split this dataset into training and testing subsets. The training set is used to train the neural net, and the testing set is kept hidden from the neural net during training, so we can use it to check the network’s performance later."
      ],
      "metadata": {
        "id": "lbmSWKUmuCFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First four columns are features\n",
        "features_train = survey[:,0:4]\n",
        "# Last column is our label\n",
        "labels_train = survey[:,4]\n",
        "\n",
        "# Keeping four surveys as test set\n",
        "test_survey = np.array([\n",
        "  [1, 1, 1, 0, 1],\n",
        "  [0, 0, 0, 1, 0],\n",
        "  [0, 0, 1, 0, 0],\n",
        "  [1, 0, 1, 0, 1]\n",
        "])\n",
        "\n",
        "features_test = test_survey[:, 0:4]\n",
        "labels_test = test_survey[:,4]"
      ],
      "metadata": {
        "id": "z87fH98ruFCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKQca9MxDgjQ"
      },
      "source": [
        "# Step 2. Build and train classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6klW7bgFjCa"
      },
      "source": [
        "Next, let's build a neural net and train it to help us make our predictions. We'll first import some needed packages and disable warnings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from warnings import filterwarnings\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "GJAt6rJDv1UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a refresher, this neural net has an input layer for features, *x* number of hidden layers to learn representations, and an output layer to make a prediction. The hidden layers find relationships between the features that help it make accurate predictions.\n",
        "\n",
        "Let's start with a simple structure: \n",
        "*   four input features\n",
        "*   one hidden layer (with four neurons, the same size as our input)\n",
        "*   two outputs (YES == satisfied, or NO == not satisfied)\n",
        "\n",
        "The sklearn package took care of counting the size of our input and output automatically, so we only need to specify the hidden layers in the code. \n",
        "\n",
        "Over the span of one iteration (or \"epoch\") of training this neural net, the hidden layer will pick up on patterns in the input features, and pass a prediction to one of two output neurons. Over multiple epochs of the same training dataset, the neural network’s predictions should keep getting better! Let's go with 1000 epochs for now."
      ],
      "metadata": {
        "id": "iquhvhxWwsXw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVzQqjoprfAB",
        "outputId": "12572701-6082-4ff3-eb46-3a3f568f1891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define the model\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(4,),  \n",
        "                    activation='tanh',        \n",
        "                    max_iter=1000,            \n",
        "                    random_state=1   \n",
        "                   )\n",
        "\n",
        "# Train the model\n",
        "mlp.fit(features_train, labels_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='tanh', hidden_layer_sizes=(4,), max_iter=1000,\n",
              "              random_state=1)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7g8uQWiFdsC"
      },
      "source": [
        "Next, we'll test our classifier on the training and testing sets to see how well it captured the information and how good its predictions are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFyuDNzCFXyZ"
      },
      "source": [
        "print(\"Training set score: %f\" % mlp.score(features_train, labels_train))\n",
        "print(\"Testing set score: %f\" % mlp.score(features_test, labels_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvr79nxCF6hY"
      },
      "source": [
        "# Step 3. Make predictions\n",
        "\n",
        "It seems like this project is almost over! Everything was simple to do, and the performance looks perfect. Now we can just input some program features find out if it's eligible to recieve funding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mYm4bONxMGM"
      },
      "source": [
        "Should we fund a program based in an urban settings that **does not** offer care services online, but offers care that is is patient-centered, culturally responsive, and timely?\n",
        "\n",
        "```[[0, 1, 1, 1]]```\n",
        "\n",
        "*   Offered online = 0\n",
        "*   Patient-centered = 1\n",
        "*   Culturally responsive = 1\n",
        "*   Timely = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBvQOnfNxGAJ"
      },
      "source": [
        "# Offered online, Patient-centered, Culturally responsive, Timely\n",
        "features = [[0, 1, 1, 1]]\n",
        "print(\"Yes, fund the program!\" if mlp.predict(features)[0] else \"Deny funding application.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh0B3BaCxPmD"
      },
      "source": [
        "Huh... okay. Let's try another one. Should we fund a program based in a rural setting that **does** offer care services online, but **does not** offer care that is is patient-centered, culturally responsive, and timely?\n",
        "\n",
        " ```[[1, 0, 0, 0]]```\n",
        "\n",
        "*   Offered online = 1\n",
        "*   Patient-centered = 0\n",
        "*   Culturally responsive = 0\n",
        "*   Timely = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsmdXpmXxRgx"
      },
      "source": [
        "# Offered online, Patient-centered, Culturally responsive, Timely\n",
        "features = [[1, 0, 0, 0]]\n",
        "print(\"Yes, fund the program!\" if mlp.predict(features)[0] else \"Deny funding application.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPIuifGrKLoq"
      },
      "source": [
        "# Step 4.  Data analysis\n",
        "\n",
        "That doesn't seem right... It looks like the neural net is biased toward programs which frequently offer care services online, even if the services rendered are *not* patient-centered,  culturally responsive, or timely.\n",
        "\n",
        "While it may be tempting to blame the algorithm, *we* collected the survey data and built the classifier, so if something went wrong and introduced bias… **it’s on us**. Let's audit our model to figure out what happened.\n",
        "\n",
        "Because we have a small set of features, we can easily visualize the correlations between them. To do this, we'll use the following libraries that we imported earlier with our packages:\n",
        "```\n",
        "matplotlib, seaborn, pandas\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6-81MHKxa1m"
      },
      "source": [
        "Earlier, we decided to pool all the survey results together. Let's disaggregate them by feature (i.e., split them up) and create plots that compare:\n",
        "1.   The proportion of surveyed S1 patients who were satisfied\n",
        "2.   The proportion of surveyed S2 clients who were satisfied\n",
        "3.   The proportion of all the people surveyed who are happy with their service, regardless of which service it was\n",
        "\n",
        "To do this, we'll  compute the number of satisfied S1 patients divided by the total number of S1 patients, the number of satisfied S2 patients divided by the total number of S2 patients, and the number of satisfied patients divided by the total number of patients surveyed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVrXT-R6xris"
      },
      "source": [
        "# Split the survey up into the S1 and S1 respondents\n",
        "S1_survey = survey[:-4]  \n",
        "S2_survey = survey[-4:]\n",
        "\n",
        "# Import plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "ind = np.arange(1, 4)\n",
        "\n",
        "# Add up the number of patients who are satisfied and divide by the total number of patients of each type\n",
        "sat_S1 = 100*np.sum(S1_survey,axis=0)[-1]/S1_survey.shape[0]\n",
        "sat_S2 = 100*np.sum(S2_survey,axis=0)[-1]/S2_survey.shape[0]\n",
        "sat = 100*np.sum(survey, axis=0)[-1]/survey.shape[0]\n",
        "\n",
        "# Make a bar chart\n",
        "pt, pd, pc = plt.bar(ind, (sat, sat_S1, sat_S2))\n",
        "\n",
        "# Assign colors to bars\n",
        "pt.set_facecolor('b')\n",
        "pd.set_facecolor('r')\n",
        "pc.set_facecolor('g')\n",
        "\n",
        "# Label plot\n",
        "ax.set_xticks(ind)\n",
        "ax.set_xticklabels(['Satisfied', 'Satisfied | S1', 'Satisfied | S2'])\n",
        "ax.set_ylim([0, 100])\n",
        "ax.set_ylabel('Percent')\n",
        "_ = ax.set_title('Service Satisfaction')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpZtLy5hxwcy"
      },
      "source": [
        "Well, that's confusing. According to our survey results, 100% of S2 patients are satisfied and the S1 patients were split on satisfaction. But when we input features for other service entities based in urban areas, our classifier will typically say that patients will not be satisfied. What's going on here?\n",
        "\n",
        "Let's look at a different dimension of our data and plot the total number of survey responses from S1 and S2 patients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeNYRBaEy5Ao"
      },
      "source": [
        "# Import library to make plots\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "ind = np.arange(1, 3)\n",
        "\n",
        "# Count the number of responses from dog vs cat owners\n",
        "S1 = S1_survey.shape[0]\n",
        "S2 = S2_survey.shape[0]\n",
        "\n",
        "# Make a bar chart\n",
        "pd, pc = plt.bar(ind, (S1, S2))\n",
        "\n",
        "# Assign colors to bars\n",
        "pd.set_facecolor('r')\n",
        "pc.set_facecolor('g')\n",
        "\n",
        "# Put labels on everything\n",
        "ax.set_xticks(ind)\n",
        "ax.set_xticklabels(['# S1', '# S2'])\n",
        "ax.set_ylim([0, 25])\n",
        "ax.set_ylabel('Number')\n",
        "_ = ax.set_title('Which Service Program?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyqP_yoQMd8M"
      },
      "source": [
        "Uh oh! Most of our survey respondents were S1 patients.\n",
        "\n",
        "**Mistake #1:** We sampled from an S1-biased set and did not check the distribution before making our model, so our dataset was *not* appropriately representative.\n",
        "\n",
        "This is something we can address through a more robust sampling or weighting methodology.\n",
        "\n",
        "But this still doesn’t fully answer why the model seems so biased against service entities in urban settings. Both S2 and S1 offer services that may be rendered online, are patient-centered, ulturally responsive, and timely.\n",
        "\n",
        "Let's take a closer look at this. We'll plot how often each feature is true for both S1 and S2 by dividing the number of times each feature is true for each service by the total number of survey responses we have for each service.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV2l9WMBMaZD"
      },
      "source": [
        "# Import libraries to build a plot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
        "ind = np.arange(0, 4)\n",
        "\n",
        "# Count number of dog and cat surveys\n",
        "total_S1 = S1_survey.shape[0]\n",
        "total_S2 = S2_survey.shape[0]\n",
        "\n",
        "# Count how often each feature is true divided by how many dogs and cats we have\n",
        "S2_prob = 100*S2_survey[:,:4].sum(axis=0)/total_S2\n",
        "S1_prob = 100*S1_survey[:,:4].sum(axis=0)/total_S1\n",
        "\n",
        "# Input the data into a bar plot\n",
        "data = {'Feature':[], 'Service':[], 'Probability':[]}\n",
        "for feature in range(4):\n",
        "  data['Feature'].append(feature)\n",
        "  data['Service'].append('S1')\n",
        "  data['Probability'].append(S1_prob[feature])\n",
        "\n",
        "  data['Feature'].append(feature)\n",
        "  data['Service'].append('S2')\n",
        "  data['Probability'].append(S2_prob[feature])\n",
        "df = pd.DataFrame(data=data)\n",
        "\n",
        "_ = sns.barplot(x='Feature', y='Probability', hue='Service', data=df, ax=ax)\n",
        "\n",
        "# Label plot\n",
        "ax.set_xticklabels(['Offered online', 'Patient-centered', 'Culturally responsive', 'Timely'])\n",
        "ax.tick_params(axis = 'both', which = 'major', labelsize = 12)\n",
        "_ = fig.suptitle('Service Entity Care Characteristics (based on patient feedback)', fontsize=20)\n",
        "_ = plt.ylabel('Probability', fontsize=18)\n",
        "_ = ax.set_ylim([0, 100])\n",
        "_ = plt.xlabel('Features', fontsize=18)\n",
        "_ = plt.legend(loc='best', prop={'size':18})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTHWwKIoUeGK"
      },
      "source": [
        "Although we know that both entities offer telehealth services, it turns out that none of the S2 patients in our sample were offered care frequently online via telehealth in the last year. This turned out to be a **correlated feature:** a feature that is unintentionally correlated to a specific prediction or hidden category. \n",
        "\n",
        "In this case, knowing if care services are frequently offered online is a subtle *cheat* for knowing that the entity is based in an urban area, even though we didn’t tell the model this.\n",
        "\n",
        "Because there were no data available to tell it otherwise, the model might have interpreted these data to imply that if care frequently offered online leads to patient satisfaction. Let's take a closer look at this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXagWEITQs7X"
      },
      "source": [
        "# Import libraries for a plot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
        "ind = np.arange(0, 4)\n",
        "\n",
        "# Count how often each respondent receives care online or in-person\n",
        "online = [0,0]\n",
        "online_count = 0\n",
        "offline = [0,0]\n",
        "offline_count = 0\n",
        "\n",
        "for entry in survey:\n",
        "  if entry[0] == 0:\n",
        "    offline[entry[-1]] += 1\n",
        "    offline_count += 1\n",
        "  else:\n",
        "    online[entry[-1]] += 1\n",
        "    online_count += 1\n",
        "\n",
        "# Put the values in a a database\n",
        "data = {'Feature':[], 'Satisfied':[], 'Probability':[]}\n",
        "data[\"Feature\"].append(\"Online\")\n",
        "data[\"Satisfied\"].append(\"Not satisfied\")\n",
        "data[\"Probability\"].append(100*online[0]/online_count)\n",
        "\n",
        "data[\"Feature\"].append(\"Online\")\n",
        "data[\"Satisfied\"].append(\"Satisfied\")\n",
        "data[\"Probability\"].append(100*online[1]/online_count)\n",
        "\n",
        "data[\"Feature\"].append(\"Offline\")\n",
        "data[\"Satisfied\"].append(\"Not satisfied\")\n",
        "data[\"Probability\"].append(100*offline[0]/offline_count)\n",
        "\n",
        "data[\"Feature\"].append(\"Offline\")\n",
        "data[\"Satisfied\"].append(\"Satisfied\")\n",
        "data[\"Probability\"].append(100*offline[1]/offline_count)\n",
        "\n",
        "df = pd.DataFrame(data=data)\n",
        "\n",
        "# Plot bar plot and put labels on everything\n",
        "_ = sns.barplot(x='Feature', y='Probability', hue='Satisfied', data=df, ax=ax)\n",
        "ax.set_xticklabels(['Online', 'In-Person'])\n",
        "ax.tick_params(axis = 'both', which = 'major', labelsize = 24)\n",
        "_ = fig.suptitle('Care delivery method and Satisfaction', fontsize=20)\n",
        "_ = plt.ylabel('Probability', fontsize=18)\n",
        "_ = ax.set_ylim([0, 100])\n",
        "_ = plt.xlabel('Features', fontsize=18)\n",
        "_ = plt.legend(loc='best', prop={'size':18})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKH-IV3tCRI3"
      },
      "source": [
        "In our dataset, if a health service is offered online, a person is very likely to be satisfied with it... *no matter what other features are true.* But if the service isn’t offered online, it’s a mixed bag.\n",
        "\n",
        "**Mistake #2:** Because our dataset had a correlated feature, our model found patterns that we didn’t want."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why did this happen?\n",
        "\n",
        "After speaking with subject matter experts to contextualize this correlation, we found that although S2 offers online and telehealth services for routine visits, most patients opt to be seen in person citing the entity's convenient location next to the bus stop and the expense of reliable home internet access. At S1, however, the lack of available public transportation in its rural community (and the town's recent move to subsidize internet access for residents) makes online/telehealth services more accessible to the routine patient population.\n",
        "\n",
        "Failure to account for these contextualizing nuances (as well as our apparent sampling errors) led to **misrepresentation** in our dataset and **misinterpretation** by our model."
      ],
      "metadata": {
        "id": "Mf7PuF9IplxI"
      }
    }
  ]
}